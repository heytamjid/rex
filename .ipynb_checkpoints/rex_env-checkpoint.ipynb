{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d7187a5-4b5c-4f94-9b1f-19f5970df07c",
   "metadata": {},
   "source": [
    "# Setting UP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df18d7a-e1dc-4c06-8cfc-627e4ebd192f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "print(f\"Does api_keys.env exist? {os.path.exists('./api_keys.env')}\")\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "loaded = load_dotenv(dotenv_path='./api_keys.env')\n",
    "print(f\"Was dotenv loaded successfully? {loaded}\")\n",
    "\n",
    "github_token = os.environ.get(\"GITHUB_TOKEN\")\n",
    "google_api_key = os.environ.get(\"GOOGLE_API_KEY\")\n",
    "\n",
    "print(f\"GitHub Token: {github_token}\")\n",
    "print(f\"Google API Key: {google_api_key}\")\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(dotenv_path='./api_keys.env')  # Specify the path to your file\n",
    "\n",
    "GITHUB_TOKEN = os.environ.get(\"GITHUB_TOKEN\")\n",
    "GOOGLE_API_KEY = os.environ.get(\"GOOGLE_API_KEY\")\n",
    "\n",
    "print(f\"GitHub Token: {GITHUB_TOKEN}\")\n",
    "print(f\"Google API Key: {GOOGLE_API_KEY}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7211947b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", google_api_key=GOOGLE_API_KEY)\n",
    "print(llm.invoke(\"Tell me a new joke each time\").content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75c6449",
   "metadata": {},
   "source": [
    "# Getting Repo Structur "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96376cc9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# folders we never want to recurse into\n",
    "EXCLUDED_DIRS = {\n",
    "    \"node_modules\", \"__pycache__\", \".git\",\n",
    "    \"dist\", \"build\", \"venv\", \"env\", \".vscode\",\n",
    "    \"target\", \"vendor\"\n",
    "}\n",
    "\n",
    "def parse_github_url(repo_url: str):\n",
    "    path = urlparse(repo_url).path\n",
    "    owner, repo = path.lstrip(\"/\").rstrip(\".git\").split(\"/\")[:2]\n",
    "    return owner, repo\n",
    "\n",
    "def fetch_repo_tree(repo_url: str):\n",
    "    token = os.getenv(\"GITHUB_TOKEN\")\n",
    "    if not token:\n",
    "        raise EnvironmentError(\"Set GITHUB_TOKEN\")\n",
    "\n",
    "    owner, repo = parse_github_url(repo_url)\n",
    "    headers = {\"Authorization\": f\"token {token}\"}\n",
    "    repo_api = f\"https://api.github.com/repos/{owner}/{repo}\"\n",
    "\n",
    "    # 1) get default branch\n",
    "    repo_info = requests.get(repo_api, headers=headers).json()\n",
    "    default_branch = repo_info.get(\"default_branch\", \"main\")\n",
    "\n",
    "    # 2) get commit sha for that branch\n",
    "    ref = requests.get(f\"{repo_api}/git/refs/heads/{default_branch}\", headers=headers).json()\n",
    "    print(ref)\n",
    "    sha = ref[\"object\"][\"sha\"]\n",
    "\n",
    "    # 3) fetch entire tree recursively\n",
    "    tree = requests.get(f\"{repo_api}/git/trees/{sha}?recursive=1\", headers=headers).json().get(\"tree\", [])\n",
    "    return tree\n",
    "\n",
    "def process_tree(tree):\n",
    "    # split blobs into files_by_dir and map subdirs\n",
    "    files_by_dir = defaultdict(list)\n",
    "    for item in tree:\n",
    "        if item[\"type\"] != \"blob\":\n",
    "            continue\n",
    "        dirpath, fname = os.path.split(item[\"path\"])\n",
    "        dirpath = dirpath or \".\"\n",
    "        files_by_dir[dirpath].append((fname, item.get(\"size\",0)))\n",
    "\n",
    "    subdirs = defaultdict(set)\n",
    "    for d in files_by_dir:\n",
    "        parent = os.path.dirname(d) or \".\"\n",
    "        if d != parent:\n",
    "            subdirs[parent].add(d)\n",
    "\n",
    "    # ---- 1) build DFS JSON list ----\n",
    "    dfs_list = []\n",
    "    def dfs(dirpath):\n",
    "        # files first\n",
    "        for fname, size in sorted(files_by_dir.get(dirpath, [])):\n",
    "            p = f\"{dirpath}/{fname}\" if dirpath!=\".\" else fname\n",
    "            dfs_list.append({\"path\": p, \"size\": size})\n",
    "        # then children\n",
    "        for child in sorted(subdirs.get(dirpath, [])):\n",
    "            name = os.path.basename(child)\n",
    "            if name in EXCLUDED_DIRS:\n",
    "                dfs_list.append({\"path\": child + \"/\", \"size\": 0})\n",
    "            else:\n",
    "                dfs(child)\n",
    "    dfs(\".\")\n",
    "\n",
    "    # ---- 2) build `tree`-style string ----\n",
    "    lines = []\n",
    "    def recurse(dirpath, prefix=\"\"):\n",
    "        # gather entries: dirs first, then files\n",
    "        entries = []\n",
    "        for child in sorted(subdirs.get(dirpath, [])):\n",
    "            entries.append((\"dir\", os.path.basename(child), child))\n",
    "        for fname, _ in sorted(files_by_dir.get(dirpath, [])):\n",
    "            entries.append((\"file\", fname, None))\n",
    "\n",
    "        for idx, (etype, name, childpath) in enumerate(entries):\n",
    "            last = (idx == len(entries)-1)\n",
    "            connector = \"└── \" if last else \"├── \"\n",
    "            suffix = \"/\" if etype==\"dir\" else \"\"\n",
    "            lines.append(prefix + connector + name + suffix)\n",
    "\n",
    "            if etype==\"dir\" and name not in EXCLUDED_DIRS:\n",
    "                extension = \"    \" if last else \"│   \"\n",
    "                recurse(childpath, prefix + extension)\n",
    "\n",
    "    recurse(\".\")\n",
    "    tree_str = \"\\n\".join(lines)\n",
    "\n",
    "    return dfs_list, tree_str\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    repo = \"https://github.com/heytamjid/dormitory-network/\" #add trailing slash\n",
    "    raw_tree = fetch_repo_tree(repo)\n",
    "    units_json, units_tree = process_tree(raw_tree)\n",
    "\n",
    "    # 1) output your DFS-style JSON\n",
    "    print(\"=== JSON units (DFS order) ===\")\n",
    "    units = json.dumps(units_json, indent=4)\n",
    "    print(units)\n",
    "\n",
    "    # 2) output your 'tree' text\n",
    "    print(\"\\n=== ASCII tree ===\")\n",
    "    print(units_tree)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bca64d1-70d9-4b6d-8f42-3e3a3d708853",
   "metadata": {},
   "source": [
    "# Setting Up LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f0e228-a16e-423c-97c0-17b3fbde2201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Standard Libraries ---\n",
    "import os\n",
    "import json\n",
    "import re # For more robust parsing if needed\n",
    "\n",
    "# --- Environment Variable Management ---\n",
    "# Best practice: Store your API key securely, e.g., in a .env file\n",
    "# Install python-dotenv: pip install python-dotenv\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# --- Langchain Core Imports (v0.1.x+ style) ---\n",
    "# Ensure you have installed:\n",
    "# pip install langchain langchain-google-genai google-cloud-aiplatform python-dotenv\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# --- Google Generative AI Integration ---\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# ==============================================================================\n",
    "# Configuration\n",
    "# ==============================================================================\n",
    "\n",
    "# --- Load Environment Variables ---\n",
    "# Create a .env file in the same directory with:\n",
    "# GOOGLE_API_KEY=\"YOUR_GEMINI_API_KEY\"\n",
    "load_dotenv()\n",
    "\n",
    "# Check if the API key is loaded (optional but good practice)\n",
    "if not os.getenv(\"GOOGLE_API_KEY\"):\n",
    "    print(\"Warning: GOOGLE_API_KEY not found in environment variables.\")\n",
    "    # You might want to exit or raise an error here in a real application\n",
    "    # For notebook use, it might still work if google-cloud-aiplatform is configured\n",
    "\n",
    "# --- LLM Configuration ---\n",
    "# Choose the Gemini model you want to use. Check Google AI documentation for available models.\n",
    "# Examples: \"gemini-1.5-flash\", \"gemini-1.0-pro\", \"gemini-pro\"\n",
    "LLM_MODEL_NAME = \"gemini-1.5-flash\"\n",
    "LLM_TEMPERATURE_IDENTIFY = 0.1 # Lower temp for more deterministic file selection\n",
    "LLM_TEMPERATURE_GENERATE = 0.4 # Higher temp for more creative documentation generation\n",
    "\n",
    "# --- File Paths ---\n",
    "REPO_FILE_LIST_JSON = 'repo_files.json' # Path to your JSON file with file list and sizes\n",
    "# !!! IMPORTANT: Set this to the base path of your *cloned* repository !!!\n",
    "# This is needed for the get_file_content function below.\n",
    "REPO_LOCAL_PATH = repo # <--- CHANGE THIS\n",
    "\n",
    "# --- Documentation Categories ---\n",
    "CATEGORIES = [\n",
    "    \"Purpose & Scope\",\n",
    "    \"System Architecture Overview\",\n",
    "    \"Core Components (Implementation Details)\",\n",
    "    \"Data Model\"\n",
    "]\n",
    "\n",
    "# --- Context Size Limits (to avoid exceeding LLM token limits) ---\n",
    "# Adjust these based on the model and your needs\n",
    "MAX_FILE_SIZE_CHARS_FOR_CONTEXT = 10000 # Max characters to read from a single file\n",
    "MAX_TOTAL_CONTEXT_CHARS = 30000         # Max total characters for context per category\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973ceea5-f11f-461f-afca-4c7bd008b42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    llm_identify = ChatGoogleGenerativeAI(\n",
    "        model=LLM_MODEL_NAME,\n",
    "        temperature=LLM_TEMPERATURE_IDENTIFY,\n",
    "        convert_system_message_to_human=True # Often helpful for Gemini models\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing Gemini LLM for identification: {e}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da065664-a83b-428d-a0cc-52693a3622ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file_list_prompt_str = units\n",
    "print(file_list_prompt_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55efcfc-7834-4b46-80a9-8e423e0e1ea7",
   "metadata": {},
   "source": [
    "# Initial Prompt to LLM to Mark Important Files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480b3a03-54fb-4b82-917c-fddfa5aa7902",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prompt_template_files = \"\"\"\n",
    "You are an expert software architect analyzing a GitHub repository's file structure.\n",
    "Your goal is to identify the *most important and relevant* files or folders from a codebase for understanding each of the following categories.\n",
    "\n",
    "Repository Tree of the codebase:\n",
    "{file_list_tree}\n",
    "\n",
    "Repository File Structure in DFS Manner (Path and Size) of the codebase in JSON format:\n",
    "{file_list}\n",
    "\n",
    "Based on the file paths and your general knowlege on codebase structure and sizes provided (i.e. you know that views.py, consumers.py etc contain the core business logic of a Django project), list the most relevant file paths for each category below.\n",
    "Prioritize files that likely contain defining information (e.g., main entry points or core modules for components, model definitions for data model).\n",
    "\n",
    "Categories:\n",
    "1.  **Purpose & Scope**: (README, high-level documentation, main application files)\n",
    "2.  **System Architecture Overview**: (Configuration files, main application/server files, core module directories, docker-compose.yml, infrastructure-as-code files, documentation about architecture, routing files )\n",
    "3.  **Core Components (Implementation Details)**: (Source code directories like 'src/', 'lib/', 'app/', key modules, files with significant size suggesting core logic)\n",
    "4.  **Data Model**: (Files named 'models.py', 'schemas.py', database interaction layers, ORM definitions, files in 'db/' or 'database/' folders)\n",
    "\n",
    "Provide your answer *strictly* in the following format, listing the file paths under each category heading. Do not add any explanation before or after the list.\n",
    "\n",
    "**Purpose & Scope:**\n",
    "- path/to/relevant/file1.ext\n",
    "- path/to/relevant/folder/\n",
    "\n",
    "**System Architecture Overview:**\n",
    "- path/to/relevant/file2.ext\n",
    "- another/relevant/path/\n",
    "\n",
    "**Core Components (Implementation Details):**\n",
    "- path/to/core/logic.py\n",
    "- src/module/\n",
    "\n",
    "**Data Model:**\n",
    "- path/to/models.py\n",
    "- database/schema/\n",
    "\"\"\"\n",
    "\n",
    "file_identification_prompt = ChatPromptTemplate.from_template(prompt_template_files)\n",
    "\n",
    "# --- Create Chain and Invoke LLM ---\n",
    "chain_identify = file_identification_prompt | llm_identify | StrOutputParser()\n",
    "\n",
    "print(\"Asking Gemini to identify important files...\")\n",
    "try:\n",
    "    llm_response_files = chain_identify.invoke({\"file_list\": file_list_prompt_str,\"file_list_tree\": units_tree})\n",
    "    print(\"\\n--- Gemini Response (Important Files Raw) ---\")\n",
    "    print(llm_response_files)\n",
    "    print(\"---------------------------------------------\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"Error invoking LLM for file identification: {e}\")\n",
    "    # Decide how to proceed - maybe use default files or exit\n",
    "    llm_response_files = \"\" # Ensure variable exists\n",
    "\n",
    "# --- Parse the LLM Response ---\n",
    "identified_files_by_category = {category: [] for category in CATEGORIES}\n",
    "current_category_key = None\n",
    "\n",
    "# Map the prompt headers to the keys in our dictionary\n",
    "# This makes parsing slightly more robust to minor variations\n",
    "category_header_map = {\n",
    "    \"**Purpose & Scope:**\": \"Purpose & Scope\",\n",
    "    \"**System Architecture Overview:**\": \"System Architecture Overview\",\n",
    "    \"**Core Components (Implementation Details):**\": \"Core Components (Implementation Details)\",\n",
    "    \"**Data Model:**\": \"Data Model\"\n",
    "}\n",
    "\n",
    "if llm_response_files:\n",
    "    lines = llm_response_files.strip().split('\\n')\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line: # Skip empty lines\n",
    "            continue\n",
    "\n",
    "        # Check if the line is a category header\n",
    "        is_header = False\n",
    "        for header, key in category_header_map.items():\n",
    "            if line.startswith(header):\n",
    "                current_category_key = key\n",
    "                is_header = True\n",
    "                break\n",
    "\n",
    "        # If it's not a header and we have a current category, assume it's a file path\n",
    "        if not is_header and current_category_key and line.startswith('-'):\n",
    "            # Extract file path: remove leading '-', trim whitespace\n",
    "            file_path = line[1:].strip()\n",
    "            if file_path: # Avoid adding empty entries\n",
    "                identified_files_by_category[current_category_key].append(file_path)\n",
    "else:\n",
    "    print(\"Warning: LLM response for file identification was empty. Cannot proceed with documentation generation.\")\n",
    "    # Optionally, define default files per category here as a fallback\n",
    "\n",
    "print(\"--- Parsed Important Files ---\")\n",
    "for category, files in identified_files_by_category.items():\n",
    "    print(f\"Category: {category}\")\n",
    "    if files:\n",
    "        for f in files:\n",
    "            print(f\"  - {f}\")\n",
    "    else:\n",
    "        print(\"  (No specific files identified for this category)\")\n",
    "print(\"----------------------------\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57ef3bf-5a5e-4a8e-b993-51e589c6721c",
   "metadata": {},
   "source": [
    "# Getting Files from Github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2cc6a2-ec1f-4e3a-83b7-c2bceba4a7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import requests\n",
    "import base64\n",
    "\n",
    "# Cap how many characters we pull per file\n",
    "MAX_FILE_SIZE_CHARS_FOR_CONTEXT = 100_000\n",
    "\n",
    "def get_file_content(repo_base_path: str, relative_file_path: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Fetches a file’s contents from GitHub via the REST API.\n",
    "\n",
    "    Args:\n",
    "        repo_base_path (str): GitHub repo URL, e.g.\n",
    "            \"https://github.com/owner/repo/\" or \"https://github.com/owner/repo\"\n",
    "        relative_file_path (str): Path within the repo, e.g. \"README.md\" or \"src/app.py\"\n",
    "\n",
    "    Returns:\n",
    "        str | None: The file’s UTF-8 text (possibly truncated), or None if not found/error.\n",
    "    \"\"\"\n",
    "    # grab your token\n",
    "    token = os.getenv(\"GITHUB_TOKEN\")\n",
    "    if not token:\n",
    "        print(\"Error: GITHUB_TOKEN environment variable is not set.\")\n",
    "        return None\n",
    "\n",
    "    # normalize the URL and extract owner/repo\n",
    "    repo_url = repo_base_path.rstrip(\"/\")  # drop trailing slash if any\n",
    "    m = re.match(r\".*github\\.com[:/](?P<owner>[^/]+)/(?P<repo>[^/]+)$\", repo_url)\n",
    "    if not m:\n",
    "        print(f\"Warning: couldn’t parse owner/repo from '{repo_base_path}'\")\n",
    "        return None\n",
    "    owner, repo = m.group(\"owner\"), m.group(\"repo\")\n",
    "\n",
    "    # build the API endpoint\n",
    "    api_url = f\"https://api.github.com/repos/{owner}/{repo}/contents/{relative_file_path.lstrip('/')}\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"token {token}\",\n",
    "        \"Accept\": \"application/vnd.github.v3+json\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        resp = requests.get(api_url, headers=headers, timeout=10)\n",
    "        if resp.status_code == 404:\n",
    "            print(f\"Warning: File not found on GitHub: {relative_file_path}\")\n",
    "            return None\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "\n",
    "        # make sure it really is a file\n",
    "        if data.get(\"type\") != \"file\" or \"content\" not in data:\n",
    "            print(f\"Skipping non-file entry: {relative_file_path}\")\n",
    "            return None\n",
    "\n",
    "        # decode Base64 payload\n",
    "        raw_bytes = base64.b64decode(data[\"content\"])\n",
    "        text = raw_bytes.decode(\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "        # truncate if too big\n",
    "        if len(text) > MAX_FILE_SIZE_CHARS_FOR_CONTEXT:\n",
    "            print(f\"Note: Content truncated at {MAX_FILE_SIZE_CHARS_FOR_CONTEXT} chars.\")\n",
    "            return text[:MAX_FILE_SIZE_CHARS_FOR_CONTEXT]\n",
    "\n",
    "        return text\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Warning: GitHub API error fetching {relative_file_path}: {e}\")\n",
    "        return None\n",
    "    except (ValueError, KeyError) as e:\n",
    "        print(f\"Warning: Unexpected API response for {relative_file_path}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8774f15d-506d-488b-9f6e-c0f754895e41",
   "metadata": {},
   "source": [
    "# Generating Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ba0909-ad35-4a3d-b85c-eb3015a159a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 2: Fetch Content and Generate Documentation using LLM\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n--- Step 2: Generating Documentation from File Content ---\")\n",
    "\n",
    "generated_documentation = {}\n",
    "\n",
    "# --- Initialize LLM for Content Generation ---\n",
    "try:\n",
    "    llm_generate = ChatGoogleGenerativeAI(\n",
    "        model=LLM_MODEL_NAME,\n",
    "        temperature=LLM_TEMPERATURE_GENERATE,\n",
    "        convert_system_message_to_human=True # Often helpful for Gemini models\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing Gemini LLM for generation: {e}\")\n",
    "    exit() # Cannot proceed without the LLM\n",
    "\n",
    "# --- Define Prompt Templates for Each Category ---\n",
    "\n",
    "# System message provides overall context and instructions\n",
    "system_prompt_content_generation = \"\"\"\n",
    "You are an expert technical writer specializing in software documentation.\n",
    "Your task is to generate clear, concise, and accurate documentation for a specific aspect of a GitHub repository.\n",
    "You will be given relevant code snippets and file paths as context. Use *only* this provided context to formulate your response.\n",
    "- Explain the requested aspect accurately based *only* on the provided snippets.\n",
    "- If the context is insufficient to answer fully, state that clearly. Do not invent information.\n",
    "- If asked for diagrams (like Mermaid), generate the code block correctly formatted (e.g., ```mermaid ... ```). Ensure the diagram reflects the relationships described in the text, based *only* on the provided context.\n",
    "- Structure your response clearly, using Markdown formatting (headings, lists, code blocks) where appropriate.\n",
    "\"\"\"\n",
    "\n",
    "# Specific prompts for each category\n",
    "prompt_templates_generate = {\n",
    "    \"Purpose & Scope\": ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt_content_generation),\n",
    "        (\"human\", \"\"\"\n",
    "        Based *only* on the following context (code snippets and file paths):\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        ---\n",
    "        Task: Provide a high-level summary of this repository's main **Purpose and Scope**.\n",
    "        - What problem does it seem to solve?\n",
    "        - Who might the intended users be?\n",
    "        - What are its key features or functionalities suggested by the provided context?\n",
    "        \"\"\")\n",
    "    ]),\n",
    "\n",
    "    \"System Architecture Overview\": ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt_content_generation + \"\\nFocus on high-level components and their interactions as suggested by the context. Use Mermaid syntax for diagrams if the context supports it.\"),\n",
    "        (\"human\", \"\"\"\n",
    "        Based *only* on the following context (code snippets and file paths):\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        ---\n",
    "        Task: Describe the overall **System Architecture**.\n",
    "        - Identify the main components suggested by the context (e.g., API, database, workers, UI, specific modules).\n",
    "        - Briefly explain their likely responsibilities based on the snippets.\n",
    "        - Describe how they might interact, according to the context.\n",
    "        - If the context provides enough information about component relationships, generate a Mermaid diagram (e.g., using `graph TD`, `sequenceDiagram`, or `componentDiagram` within ```mermaid ... ``` tags) to visualize this. If not, state that a diagram cannot be generated from the provided context.\n",
    "        \"\"\")\n",
    "    ]),\n",
    "\n",
    "    \"Core Components (Implementation Details)\": ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt_content_generation + \"\\nFocus on explaining the 'what', 'how', and 'where' of key functionalities based *only* on the provided snippets.\"),\n",
    "        (\"human\", \"\"\"\n",
    "        Based *only* on the following context (code snippets and file paths):\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        ---\n",
    "        Task: Identify and describe the **Core Components or Modules** suggested by the context.\n",
    "        For each key component or functionality hinted at in the snippets:\n",
    "        1.  **What it does:** Its apparent primary function or responsibility.\n",
    "        2.  **How it works:** Key logic, algorithms, or patterns visible in the snippets (briefly).\n",
    "        3.  **Where it's implemented:** The specific file(s) or function(s) from the context responsible for this logic.\n",
    "        Structure your answer clearly, perhaps using headings for each identified component. If the context is insufficient for a component, state that.\n",
    "        \"\"\")\n",
    "    ]),\n",
    "\n",
    "    \"Data Model\": ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt_content_generation + \"\\nFocus on data structures, storage, and relationships visible in the context. Use Mermaid syntax for diagrams if appropriate and supported by the context.\"),\n",
    "        (\"human\", \"\"\"\n",
    "        Based *only* on the following context (code snippets and file paths), paying close attention to data definitions and interactions:\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        ---\n",
    "        Task: Describe the main **Data Model** apparent in the application.\n",
    "        Look for and describe (if present in the context):\n",
    "        - Database schemas (tables, columns, relationships suggested by model definitions or SQL).\n",
    "        - Key data structures or objects (e.g., classes, interfaces, dictionaries) used for data representation.\n",
    "        - Data validation rules mentioned.\n",
    "        - Interaction with data storage (e.g., ORM usage, database connection patterns).\n",
    "        - If the context clearly defines primary data entities and their relationships, represent them using Mermaid syntax (e.g., ```mermaid erDiagram ... ```). If not, state that a diagram cannot be generated from the provided context.\n",
    "        \"\"\")\n",
    "    ])\n",
    "}\n",
    "\n",
    "\n",
    "# --- Generate Documentation for Each Category ---\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "for category, files in identified_files_by_category.items():\n",
    "    print(f\"\\n--- Generating documentation for: {category} ---\")\n",
    "    if not files:\n",
    "        print(\"No relevant files were identified for this category by the previous step. Skipping generation.\")\n",
    "        generated_documentation[category] = \"No files identified as relevant by the initial analysis.\"\n",
    "        continue\n",
    "\n",
    "    # --- Build Context from File Contents ---\n",
    "    context_str = \"\"\n",
    "    current_total_chars = 0\n",
    "    files_included_in_context = []\n",
    "\n",
    "    print(f\"Attempting to fetch content for: {', '.join(files)}\")\n",
    "    for file_path in files:\n",
    "        if current_total_chars >= MAX_TOTAL_CONTEXT_CHARS:\n",
    "            print(f\"Note: Reached max total context size ({MAX_TOTAL_CONTEXT_CHARS} chars). Skipping remaining files for {category}.\")\n",
    "            break\n",
    "\n",
    "        # Use the helper function to get conten\n",
    "        print(REPO_LOCAL_PATH)\n",
    "        print(file_path)\n",
    "        content = get_file_content(REPO_LOCAL_PATH, file_path)\n",
    "\n",
    "        if content:\n",
    "            snippet = f\"\\n\\n--- Start of content from: {file_path} ---\\n\"\n",
    "            snippet += content\n",
    "            snippet += f\"\\n--- End of content from: {file_path} ---\\n\"\n",
    "\n",
    "            if current_total_chars + len(snippet) <= MAX_TOTAL_CONTEXT_CHARS:\n",
    "                context_str += snippet\n",
    "                current_total_chars += len(snippet)\n",
    "                files_included_in_context.append(file_path)\n",
    "                # print(f\"Added content from {file_path} ({len(content)} chars)\")\n",
    "            else:\n",
    "                # Try adding a truncated version if the full snippet exceeds the *remaining* space\n",
    "                remaining_space = MAX_TOTAL_CONTEXT_CHARS - current_total_chars\n",
    "                if remaining_space > 100: # Only add if there's meaningful space left\n",
    "                    truncated_content = content[:remaining_space - 100] # Leave space for headers/footers\n",
    "                    snippet = f\"\\n\\n--- Start of truncated content from: {file_path} ---\\n\"\n",
    "                    snippet += truncated_content\n",
    "                    snippet += f\"\\n--- End of truncated content from: {file_path} ---\\n\"\n",
    "                    context_str += snippet\n",
    "                    current_total_chars += len(snippet)\n",
    "                    files_included_in_context.append(f\"{file_path} (truncated)\")\n",
    "                    print(f\"Note: Added truncated content from {file_path} to fit context limit.\")\n",
    "                # Stop adding more files once the limit is hit, even with truncation\n",
    "                print(f\"Note: Reached max total context size ({MAX_TOTAL_CONTEXT_CHARS} chars). Skipping remaining files for {category}.\")\n",
    "                break\n",
    "        # else: File not found or error message already printed by get_file_content\n",
    "\n",
    "    if not context_str:\n",
    "         print(f\"Could not retrieve any content for the identified files for {category}. Skipping generation.\")\n",
    "         generated_documentation[category] = \"Failed to retrieve content for the files identified as relevant.\"\n",
    "         continue\n",
    "\n",
    "    print(f\"Built context for {category} using files: {', '.join(files_included_in_context)} ({current_total_chars} chars)\")\n",
    "\n",
    "    # --- Get Prompt and Create Chain ---\n",
    "    prompt_template = prompt_templates_generate.get(category)\n",
    "    if not prompt_template:\n",
    "        print(f\"Warning: No prompt template defined for category '{category}'. Skipping.\")\n",
    "        generated_documentation[category] = f\"Internal Error: No prompt template for category.\"\n",
    "        continue\n",
    "\n",
    "    chain_generate = prompt_template | llm_generate | output_parser\n",
    "\n",
    "    # --- Invoke LLM for Documentation Generation ---\n",
    "    try:\n",
    "        print(f\"Invoking LLM to generate documentation for {category}...\")\n",
    "        category_documentation = chain_generate.invoke({\"context\": context_str})\n",
    "        generated_documentation[category] = category_documentation\n",
    "        print(f\"--- Successfully generated documentation for: {category} ---\")\n",
    "        # print(category_documentation[:500] + \"...\") # Print start of generated doc\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating documentation for {category}: {e}\")\n",
    "        generated_documentation[category] = f\"Error during generation: {e}\"\n",
    "\n",
    "# ==============================================================================\n",
    "# Final Output\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\\n\" + \"=\"*60)\n",
    "print(\"      FINAL GENERATED REPOSITORY DOCUMENTATION\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "for category, doc in generated_documentation.items():\n",
    "    print(f\"\\n## {category}\\n\")\n",
    "    print(doc if doc else \"(No documentation generated for this category)\")\n",
    "    print(\"\\n\" + \"-\"*40 + \"\\n\")\n",
    "\n",
    "# --- Optional: Save to files ---\n",
    "# output_dir = \"generated_docs\"\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "# for category, doc in generated_documentation.items():\n",
    "#     filename = category.lower().replace(' ', '_').replace('&', 'and').replace('(', '').replace(')', '') + \".md\"\n",
    "#     filepath = os.path.join(output_dir, filename)\n",
    "#     try:\n",
    "#         with open(filepath, 'w', encoding='utf-8') as f:\n",
    "#             f.write(f\"# {category}\\n\\n{doc}\")\n",
    "#         print(f\"Saved documentation for {category} to {filepath}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error saving documentation for {category} to {filepath}: {e}\")\n",
    "\n",
    "print(\"\\n--- Script Finished ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccaed0dd-c480-4991-b9f5-b5b97e8d0112",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\n\" + \"=\"*60)\n",
    "print(\"      FINAL GENERATED REPOSITORY DOCUMENTATION\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "for category, doc in generated_documentation.items():\n",
    "    print(f\"\\n## {category}\\n\")\n",
    "    print(doc if doc else \"(No documentation generated for this category)\")\n",
    "    print(\"\\n\" + \"-\"*40 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
